# Brewery Datalake Pipeline ğŸº

Pipeline de dados completo utilizando Apache Airflow + Docker.

Arquitetura Bronze â†’ Silver â†’ Gold com:

- IngestÃ£o via API OpenBrewery
- TransformaÃ§Ãµes em Pandas
- OrquestraÃ§Ã£o Airflow
- Auditoria Postgres
- Quality Checks
- Testes automatizados
- Makefile

---

## ğŸ§± Arquitetura
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚        Developer            â”‚
                  â”‚  (feature/* branch)         â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â†“
                       Pull Request (to develop/main)
                                â†“
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚     GitHub Actions CI      â”‚
                  â”‚  - pytest                  â”‚
                  â”‚  - lint (future)           â”‚
                  â”‚  - quality checks          â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â†“
                        Merge Approved
                                â†“
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                 DATA PIPELINE (RUNTIME)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                   Open Brewery API
               â†“ (paginated ingestion)
                  Python Extract Job
                          â†“
               Data Lake Local Storage
                 â”œâ”€â”€ Bronze (Raw)
                 â”œâ”€â”€ Silver (Clean)
                 â””â”€â”€ Gold (Analytics)
                          â†“
               Airflow Orchestration
                          â†“
            Monitoring + Alerts + Metrics
            
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

###ğŸ“‚ Estrutura do projeto
dags/        â†’ DAG Airflow
src/         â†’ lÃ³gica pipeline
tests/       â†’ pytest
datalake/    â†’ bronze/silver/gold

###ğŸ›  Stack
Apache Airflow
Docker
Postgres
Pandas
Pytest
---

## ğŸš€ Como rodar

### PrÃ©-requisitos
- GIT
- Docker
- Docker Compose
- Make

---

### 1. Clone

bash
`git clone <repo>`
`cd brewery-datalake`

### 2. Configure as variaveis editando o arquivo criando arquivo .env
`cp example_env .env`
cadastre o seu e-mail na variavel ALERT_EMAIL
Gere o cÃ³digo FERNET_KEY atravÃ©s dos comandos:
`make fernet` ou 	`python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"`
Copie a chave e cole na variavel FERNET_KEY

### 3. Suba a Stack executando os comando
`make up` ou `docker compose up -d --build`

O Airflow ficarÃ¡ disponÃ­vel em:
http://localhost:8080
user: airflow
pass: airflow

## ğŸ§ª 4. Rodar pipeline
###Para executar um pipeline execute os comandos a seguir:
`make dag` ou `docker exec -it airflow_webserver airflow dags trigger brewery_datalake_pipeline`
### VocÃª tambÃ©m pode Listar as Dags e as tasks com os comandos:
`make ls-dag` ou `docker exec -it airflow_webserver ls /opt/airflow/dags`
`make ls-task` ou `docker exec -it airflow_webserver airflow tasks list brewery_datalake_pipeline`
### Caso queira executar o pipeline para uma data especifica utilize o formato 2026-02-17 apÃ³s os comandos abaixo, caso nenhuma data seja informada serÃ¡ utilizada a data do dia
`make pipeline` ou `docker exec -it airflow_webserver airflow dags test brewery_datalake_pipeline 2026-02-17`

##ğŸ“Š 5. Rodar Testes
`make test` ou `docker exec -it airflow_scheduler bash -lc "pytest -q /opt/airflow/tests"`

ğŸ‘¤ Autor
Marco AurÃ©lio
