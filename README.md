# Brewery Datalake Pipeline ğŸº

Pipeline de dados completo utilizando Apache Airflow + Docker.

Arquitetura Bronze â†’ Silver â†’ Gold com:

- IngestÃ£o via API OpenBrewery
- TransformaÃ§Ãµes em Pandas
- OrquestraÃ§Ã£o Airflow
- Auditoria Postgres
- Quality Checks
- Testes automatizados
- Makefile

---

## ğŸ§± Arquitetura


---

## ğŸš€ Como rodar

### PrÃ©-requisitos

- Docker
- Docker Compose
- Make
---

### 1. Clone

```bash
git clone <repo>
cd brewery-datalake

### 2. Configure as variaveis editando o arquivo criando arquivo .env
cp example_env .env
cadastre o seu e-mail na variavel ALERT_EMAIL
Gere o cÃ³digo FERNET_KEY atravÃ©s dos comandos:
make fernet ou 	python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
Copie a chave e cole na variavel FERNET_KEY

### 3. Suba a Stack executando os comando
make up ou docker compose up -d --build

O Airflow ficarÃ¡ disponÃ­vel em:
http://localhost:8080

user: airflow
pass: airflow

## ğŸ§ª Rodar pipeline
make dag ou docker exec -it airflow_webserver airflow dags trigger brewery_datalake_pipeline
# VocÃª tambÃ©m pode Listar as Dags e as tasks com os comandos:
make ls-dag ou docker exec -it airflow_webserver ls /opt/airflow/dags
make ls-task ou docker exec -it airflow_webserver airflow tasks list brewery_datalake_pipeline
# Caso queira definir uma data para o pipeline deve ser no formato 2026-02-17
make pipeline ou docker exec -it airflow_webserver airflow dags test brewery_datalake_pipeline $(TODAY)

ğŸ“Š Rodar Testes
make test ou docker exec -it airflow_scheduler bash -lc "pytest -q /opt/airflow/tests"

ğŸ“‚ Estrutura do projeto
dags/        â†’ DAG Airflow
src/         â†’ lÃ³gica pipeline
tests/       â†’ pytest
datalake/    â†’ bronze/silver/gold

ğŸ›  Stack
Apache Airflow
Docker
Postgres
Pandas
Pytest

ğŸ‘¤ Autor
Marco AurÃ©lio